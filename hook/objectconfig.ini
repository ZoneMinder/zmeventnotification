# Configuration file for object detection

# NOTE: ALL parameters here can be overriden
# on a per monitor basis if you want. Just
# duplicate it inside the correct [monitor-<num>] section

[general]
# This is an optional file
# If specified, you can specify tokens with secret values in that file
# and onlt refer to the tokens in your main config file
secrets = /etc/zm/secrets.ini

# portal/user/password are needed if you plan on using ZM's legacy
# auth mechanism to get images
portal=!ZM_PORTAL
user=!ZM_USER
password=!ZM_PASSWORD

# api portal is needed if you plan to use tokens to get images
# requires ZM 1.33 or above
api_portal=!ZM_API_PORTAL

allow_self_signed=yes
# if yes, last detection will be stored for monitors
# and bounding boxes that match, along with labels
# will be discarded for new detections. This may be helpful
# in getting rid of static objects that get detected
# due to some motion. 
match_past_detections=yes
# The max difference in area between the objects if match_past_detection is on
# can also be specified in px like 300px. Default is 5%. Basically, bounding boxes of the same
# object can slightly differ ever so slightly between detection. Contributor @neillbell put in this PR
# to calculate the difference in areas and based on his tests, 5% worked well. YMMV. Change it if needed.
past_det_max_diff_area=5%

# sequence of models to run for detection
models=yolo,face
# if all, then we will loop through all models
# if first then the first success will break out
detection_mode=all

# If you need basic auth to access ZM 
#basic_user=user
#basic_password=password

# base data path for various files the ES+OD needs
# we support in config variable substitution as well
base_data_path=/var/lib/zmeventnotification

# this is the global detection pattern used for all monitors.
# choose any set of classes from here https://github.com/pjreddie/darknet/blob/master/data/coco.names
# for everything, make it .*
detect_pattern=(person|car|motorbike|bus|truck|boat)
#detect_pattern=.*

# global settings for 
# bestmatch, alarm, snapshot OR a specific frame ID
frame_id=bestmatch

# this is the to resize the image before analysis is done
resize=1200
# set to yes, if you want to remove images after analysis
# setting to yes is recommended to avoid filling up space
# keep to no while debugging/inspecting masks
# Note this does NOT delete debug images later
delete_after_analyze=yes

# If yes, will write an image called <filename>-bbox.jpg as well
# which contains the bounding boxes. This has NO relation to 
# write_image_to_zm 
# Typically, if you enable delete_after_analyze you may
# also want to set  write_debug_image to no. 
write_debug_image=no

# if yes, will write an image with bounding boxes
# this needs to be yes to be able to write a bounding box
# image to ZoneMinder that is visible from its console
write_image_to_zm=yes

# Adds percentage to detections
# hog/face shows 100% always
show_percent=yes

# color to be used to draw the polygons you specified
poly_color=(255,255,255)
#import_zm_zones=no


## Monitor specific settings
#
# - Format:  [monitor-<mid>]
#
# Parameters:
# polygon areas where object detection will be done.
# You can name them anything except the keywords defined in the optional
# params below. You can put as many polygons as you want per [monitor-<mid>]
# (see examples).
#
# detect_pattern: overrides the detection patterns used for this monitor.
#
# Examples:

[monitor-8]
# my driveway
match_past_detections=no
wait=5
detect_pattern=(person|car|motorbike|bus|truck|boat)

#alpr_pattern=^(.*x11)
#delete_after_analyze=no
#detect_pattern=.*
#import_zm_zones=yes
my_driveway_perimeter=306,356 1003,341 1074,683 154,715
# use license plate recognition for my driveway
# see alpr section later for more data needed
resize=no
models=yolo,alpr
# tiny switches to tiny yolo weights, instead of full Yolo. Much faster, but less accurate
#yolo_type=tiny

[monitor-10]
# my front lawn
# here we want anything except potted plant
# exclusion in regular expressions is not
# as straightforward as you may think, so 
# follow this pattern
# detect_pattern = ^(?!object1|object2|objectN)
# the characters in front implement what is 
# called a negative look ahead

detect_pattern=^(?!potted plant|pottedplant|bench|broccoli)
#detect_pattern=.*

# local model overrides global
models=yolo

# setting import_zm_zones to yes will import ZM defined zones
#import_zm_zones=yes

[monitor-5]
# my basement
detect_pattern=(person)
#detect_pattern=.*
#poly_color=(255,0,0)
#detect_pattern=^(?!chair|bed)
param=219,304 1113,278 1066,863 177,852
models=yolo,face

[monitor-6]
# deck
detect_pattern=^(?!chair|table|bench|bird|bicycle|frisbee)
#detect_pattern=^(?!chair|table|bench|bird)
models=yolo
#yolo_type=tiny
boundary=100,100 2988,10 2988,2220 10,2220

[monitor-2]
#doorbell
detect_pattern=(person)
#detect_pattern=.*
# try face, if it works, don't do yolo
detection_mode=first
models=face,yolo
frame_id=bestmatch
# try diff. sizes. In my case, 600 was enough
#resize=600
# My doorbell camera needs more accurate face detection
# cnn did a much better job than HOG, but its _much_ slower
face_model=cnn
face_train_model=cnn
face_recog_dist_threshold=0.6
match_past_detections=no


#if you hard code a frame, you need to make sure it is created
#before we acess it. wait (sec) helps
#frame_id=32
#wait=3



#[monitor-4]
# detect_pattern=(cat|dog)
# kitchen_door=313,221 392,210 418,592 367,659

# No 'detect_pattern', global value would be used.
# [monitor-7]
# entrance_door=313,221 392,210 418,592 367,659

# Machine learning options that are not specific to a model
[ml]
# You can now run the machine learning code on a different server
# This frees up your ZM server for other things
# To do this, you need to setup https://github.com/pliablepixels/mlapi
# on your desired server and confiure it with a user. See its instructions
# once set up, you can choose to do object/face recognition via that 
# external serer

# URL that will be used
#ml_gateway=http://192.168.1.21:5000/api/v1

# If you enable ml_gateway, and it is down
# you can set ml_fallback_local to yes
# if you want to instantiate local object detection
# on gateway failure. Default is no
#ml_fallback_local=yes

# API/password for remote gateway
ml_user=!ML_USER
ml_password=!ML_PASSWORD


# config files for yolo
[yolo]
yolo_type=full
#yolo_type=tiny

#yolo_min_confidence=0.5
yolo_min_confidence=0.3

config={{base_data_path}}/models/yolov3/yolov3.cfg
weights={{base_data_path}}/models/yolov3/yolov3.weights
labels={{base_data_path}}/models/yolov3/yolov3_classes.txt

tiny_config={{base_data_path}}/models/tinyyolo/yolov3-tiny.cfg
tiny_weights={{base_data_path}}/models/tinyyolo/yolov3-tiny.weights
tiny_labels={{base_data_path}}/models/tinyyolo/yolov3-tiny.txt

# config params for HOG
[hog]
stride=(4,4)
padding=(8,8)
scale=1.05
mean_shift=-1

[face]
# this directly will be where you store known images on a per directory basis
known_images_path={{base_data_path}}/known_faces

# if yes, then unknown faces will be stored and you can analyze them later
# and move to known_faces and retrain
save_unknown_faces=yes

# How many pixels to extend beyond the face for a better perspective
save_unknown_faces_leeway_pixels=50

# this directly is where zm_detect will store faces it could not identify
# (if save_unknown_faces is yes). You can then inspect this folder later, 
# and copy unknown faces to the right places in known_faces and retrain
unknown_images_path={{base_data_path}}/unknown_faces


# read https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems
# read https://github.com/ageitgey/face_recognition#automatically-find-all-the-faces-in-an-image
# and play around

# quick overview: 
# num_jitters is how many times to distort images 
# upsample_times is how many times to upsample input images (for small faces, for example)
# model can be hog or cnn. cnn may be more accurate, but I haven't found it to be 

face_num_jitters=1
face_model=hog
face_upsample_times=1

# This is maximum distance of the face under test to the closest matched
# face cluster. The larger this distance, larger the chances of misclassification.
#
face_recog_dist_threshold=0.6
# When we are first training the face recognition model with known faces,
# by default we use hog because we assume you will supply well lit, front facing faces
# However, if you are planning to train with profile photos or hard to see faces, you
# may want to change this to cnn. Note that this increases training time, but training only
# happens once, unless you retrain again by removing the training model
face_train_model=hog
#if a face doesn't match known names, we will detect it as 'unknown face'
# you can change that to something that suits your personality better ;-)
#unknown_face_name=invader

[alpr]

alpr_use_after_detection_only=yes

# -----| If you are using plate recognizer | ------
alpr_service=plate_recognizer
# If you want to host a local SDK https://app.platerecognizer.com/sdk/
#alpr_url=https://localhost:8080
# Plate recog replace with your api key
alpr_key=!PLATEREC_ALPR_KEY
# if yes, then it will log usage statistics of the ALPR service
platerec_stats=no
# If you want to specify regions. See http://docs.platerecognizer.com/#regions-supported
#platerec_regions=['us','cn','kr']
# minimal confidence for actually detecting a plate
platerec_min_dscore=0.1
# minimal confidence for the translated text
platerec_min_score=0.2


# ----| If you are using openALPR |-----
#alpr_service=open_alpr
#alpr_key=!OPENALPR_ALPR_KEY

# For an explanation of params, see http://doc.openalpr.com/api/?api=cloudapi
#openalpr_recognize_vehicle=1
#openalpr_country=us
#openalpr_state=ca
# openalpr returns percents, but we convert to between 0 and 1
#openalpr_min_confidence=0.3

