# Configuration file for object detection

# NOTE: ALL parameters here can be overriden
# on a per monitor basis if you want. Just
# duplicate it inside the correct [monitor-<num>] section

[general]

# Please don't change this. It is used by the config upgrade script
version=1.0

# This is an optional file
# If specified, you can specify tokens with secret values in that file
# and onlt refer to the tokens in your main config file
secrets = /etc/zm/secrets.ini


# You can now limit the # of detection process
# per target processor. If not specified, default is 1
# Other detection processes will wait to acquire lock

cpu_max_processes=3
tpu_max_processes=1
gpu_max_processes=1

# NEW: Time to wait in seconds per processor to be free, before
# erroring out. Default is 120 (2 mins)
cpu_max_lock_wait=120
tpu_max_lock_wait=120
gpu_max_lock_wait=120

# base data path for various files the ES+OD needs
# we support in config variable substitution as well
base_data_path=/var/lib/zmeventnotification

# It seems certain systems don't follow regular
# ZM conventions on install paths. This may cause 
# problems with pyzm that the hooks use to do logging
# Look at https://pyzm.readthedocs.io/en/latest/source/pyzm.html#pyzm.ZMLog.init for parameters. Default is "{}"
# You can also use this to control logging irrespective of ZM log settings
#pyzm_overrides = {'conf_path':'/etc/zm'}
pyzm_overrides={'log_level_debug':2}

# base path where ZM config files reside
# this is needed by pyzm especially if your paths are different
# default is /etc/zm
base_zm_conf_path=/etc/zm

# portal/user/password are needed if you plan on using ZM's legacy
# auth mechanism to get images
portal=!ZM_PORTAL
user=!ZM_USER
password=!ZM_PASSWORD

# api portal is needed if you plan to use tokens to get images
# requires ZM 1.33 or above
api_portal=!ZM_API_PORTAL

allow_self_signed=yes

# If specified, will limit detected object size to this amount of
# the total image size passed. Can help avoiding weird detections
# You can specify as % or px. Default is px
# Remember the image is resized to 416x416 internally. better
# to keep in %
#max_detection_size=95%


# if yes, last detection will be stored for monitors
# and bounding boxes that match, along with labels
# will be discarded for new detections. This may be helpful
# in getting rid of static objects that get detected
# due to some motion. 
match_past_detections=yes

# The max difference in area between the objects if match_past_detection is on
# can also be specified in px like 300px. Default is 5%. Basically, bounding boxes of the same
# object can slightly differ ever so slightly between detection. Contributor @neillbell put in this PR
# to calculate the difference in areas and based on his tests, 5% worked well. YMMV. Change it if needed.
past_det_max_diff_area=5%

# sequence of models to run for detection
detection_sequence=object,face

# if all, then we will loop through all models
# if first then the first success will break out
detection_mode=all

# If you need basic auth to access ZM 
#basic_user=user
#basic_password=password


# global settings for 
# bestmatch, alarm, snapshot OR a specific frame ID
frame_id=bestmatch

# Typically best match means it will first try alarm 
# and then snapshot. If you want it the reverse way, 
# make the order 's,a'. Don't get imaginative here -
# 's,a' is the only thing it understands. Everything else
# means alarm then snapshot.
#bestmatch_order = s,a

# this is the to resize the image before analysis is done
resize=1200
# set to yes, if you want to remove images after analysis
# setting to yes is recommended to avoid filling up space
# keep to no while debugging/inspecting masks
# Note this does NOT delete debug images later
delete_after_analyze=yes

# If yes, will write an image called <filename>-bbox.jpg as well
# which contains the bounding boxes. This has NO relation to 
# write_image_to_zm 
# Typically, if you enable delete_after_analyze you may
# also want to set  write_debug_image to no. 
write_debug_image=no

# if yes, will write an image with bounding boxes
# this needs to be yes to be able to write a bounding box
# image to ZoneMinder that is visible from its console
write_image_to_zm=yes

# Adds percentage to detections
# hog/face shows 100% always
show_percent=yes

# color to be used to draw the polygons you specified
poly_color=(255,255,255)
# Make this 0 if you don't want to see polygons
poly_thickness=2

# If yes, will import zones automatically from monitors
#import_zm_zones=no

# If yes, will match object detections only in areas
# that ZM recorded motion. Note that the ES will only know
# the initial zones motion was triggered in before an alarm 
# was raised. If ZM adds more zones later in the course of the event,
# the ES will NOT know

#only_triggered_zm_zones=no


[remote]

# You can now run the machine learning code on a different server
# This frees up your ZM server for other things
# To do this, you need to setup https://github.com/pliablepixels/mlapi
# on your desired server and confiure it with a user. See its instructions
# once set up, you can choose to do object/face recognition via that 
# external serer

# URL that will be used
#ml_gateway=http://192.168.1.21:5000/api/v1

# If you enable ml_gateway, and it is down
# you can set ml_fallback_local to yes
# if you want to instantiate local object detection
# on gateway failure. Default is no
#ml_fallback_local=yes

# API/password for remote gateway
#ml_user=!ML_USER
#ml_password=!ML_PASSWORD
#ml_fallback_local=no


[object]

# this is the global detection pattern used for all monitors.
# choose any set of classes from here https://github.com/pjreddie/darknet/blob/master/data/coco.names
# for everything, make it .*
object_detection_pattern=(person|car|motorbike|bus|truck|boat)
#object_detection_pattern=.*

object_min_confidence=0.3


# For Google Coral Edge TPU
#object_framework=coral_edgetpu
#object_processor=tpu
#object_weights={{base_data_path}}/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite
#object_labels={{base_data_path}}/models/coral_edgetpu/coco_indexed.names



# For YoloV3 full
object_framework=opencv
object_processor=cpu # or gpu
object_config={{base_data_path}}/models/yolov3/yolov3.cfg
object_weights={{base_data_path}}/models/yolov3/yolov3.weights
object_labels={{base_data_path}}/models/yolov3/coco.names

# FOR YoloV4. 
#object_framework=opencv
#object_processor=cpu # or gpu
#object_config={{base_data_path}}/models/yolov4/yolov4.cfg
#object_weights={{base_data_path}}/models/yolov4/yolov4.weights
#object_labels={{base_data_path}}/models/yolov4/coco.names


# For tiny Yolo V3
#object_framework=opencv
#object_processor=cpu #or gpu
#object_config={{base_data_path}}/models/tinyyolov3/yolov3-tiny.cfg
#object_weights={{base_data_path}}/models/tinyyolov3/yolov3-tiny.weights
#object_labels={{base_data_path}}/models/tinyyolov3/coco.names

# For tiny Yolo V4
#object_framework=opencv
#object_processor=cpu # or gpu
#object_config={{base_data_path}}/models/tinyyolov4/yolov4-tiny.cfg
#object_weights={{base_data_path}}/models/tinyyolov4/yolov4-tiny.weights
#object_labels={{base_data_path}}/models/tinyyolov4/coco.names


# config params for HOG
[hog]
stride=(4,4)
padding=(8,8)
scale=1.05
mean_shift=-1

[face]

# this is the global detection pattern used for all monitors.
#face_detection_pattern=(Name1|Name2|Name3)


# As of today, only dlib can be used
# Coral TPU only supports face detection
# Maybe in future, we can do different frameworks
# for detection and recognition
face_detection_framework=dlib
face_recognition_framework=dlib


# this directly will be where you store known images on a per directory basis
known_images_path={{base_data_path}}/known_faces

# if yes, then unknown faces will be stored and you can analyze them later
# and move to known_faces and retrain
save_unknown_faces=yes

# How many pixels to extend beyond the face for a better perspective
save_unknown_faces_leeway_pixels=50

# this directly is where zm_detect will store faces it could not identify
# (if save_unknown_faces is yes). You can then inspect this folder later, 
# and copy unknown faces to the right places in known_faces and retrain
unknown_images_path={{base_data_path}}/unknown_faces


# read https://github.com/ageitgey/face_recognition/wiki/Face-Recognition-Accuracy-Problems
# read https://github.com/ageitgey/face_recognition#automatically-find-all-the-faces-in-an-image
# and play around

# quick overview: 
# num_jitters is how many times to distort images 
# upsample_times is how many times to upsample input images (for small faces, for example)
# model can be hog or cnn. cnn may be more accurate, but I haven't found it to be 

face_num_jitters=1
face_model=hog
face_upsample_times=1

# This is maximum distance of the face under test to the closest matched
# face cluster. The larger this distance, larger the chances of misclassification.
#
face_recog_dist_threshold=0.6
# When we are first training the face recognition model with known faces,
# by default we use hog because we assume you will supply well lit, front facing faces
# However, if you are planning to train with profile photos or hard to see faces, you
# may want to change this to cnn. Note that this increases training time, but training only
# happens once, unless you retrain again by removing the training model
face_train_model=hog
#if a face doesn't match known names, we will detect it as 'unknown face'
# you can change that to something that suits your personality better ;-)
#unknown_face_name=invader

[alpr]


# this is the global detection pattern used for all monitors.
#alpr_detection_pattern=(licenseplate1|licenseplate2|licenseplate3)

# keep this to yes. no mode is not supported today
alpr_use_after_detection_only=yes


# plate_recognizer, open_alpr, open_alpr_cmdline
alpr_service=plate_recognizer

# Many of the ALPR providers offer both a cloud version
# and local SDK version. Sometimes local SDK format differs from
# the cloud instance. Set this to local or cloud. Default cloud
alpr_api_type=cloud

# If you want to host a local SDK https://app.platerecognizer.com/sdk/
#alpr_url=https://localhost:8080/alpr
# Plate recog replace with your api key
alpr_key=!PLATEREC_ALPR_KEY
# if yes, then it will log usage statistics of the ALPR service
platerec_stats=no
# If you want to specify regions. See http://docs.platerecognizer.com/#regions-supported
#platerec_regions=['us','cn','kr']
# minimal confidence for actually detecting a plate
platerec_min_dscore=0.1
# minimal confidence for the translated text
platerec_min_score=0.2


# ----| If you are using openALPR web service |-----
#alpr_service=open_alpr
#alpr_key=!OPENALPR_ALPR_KEY

# For an explanation of params, see http://doc.openalpr.com/api/?api=cloudapi
#openalpr_recognize_vehicle=1
#openalpr_country=us
#openalpr_state=ca
# openalpr returns percents, but we convert to between 0 and 1
#openalpr_min_confidence=0.3


# ----| If you are using openALPR command line |-----

# Before you do any of this, make sure you have openALPR
# compiled and working properly as per http://doc.openalpr.com/compiling.html
# the alpr binary needs to be operational and capable of detecting plates

# Note this is not really very accurate unless you 
# have a camera directly with a good view of the palates
# the cloud based API service is far more accurate

#openalpr_cmdline_binary=alpr

# Do an alpr -help to see options, plug them in here
# like say '-j -p ca -c US' etc.
# keep the -j because its JSON

# Note that alpr_pattern is honored
# For the rest, just stuff them in the cmd line options

#openalpr_cmdline_params=-j
#openalpr_cmdline_min_confidence=0.3





# This section gives you an option to get brief animations 
# of the event, delivered as part of the push notification to mobile devices
# Animations are created only if an object is detected

[animation]
# Seems like GIF/MP4 animations only
# work in IOS. Too bad.

# NOTE: Animation ONLY works with ZM 1.35 master as of Mar 16, 2020
# You also require zmNinja 1.3.91 or above
# If you are not running that version, animation will not work
# Animation frames will be created, but they won't be pushed to your device

# If yes, object detection will attempt to create 
# a short GIF file around the object detection frame
# that can be sent via push notifications for instant playback
# Note this required additional software support. Default:no
create_animation=no

# Format of animation burst
# valid options are "mp4", "gif", "mp4,gif"
# Note that gifs will be of a shorter duration
# as they take up much more disk space than mp4
# Note that if you use mp4, the thumbnail that shows 
# with push notifications may look transparent. My guess
# is this is related to how the video is being formed
# in ZM as it is a partial video when we process it

# Note that if you use mp4, you need to change the picture_url
# in zmeventnotification.ini to objdetect_mp4. When you use objdetect,
# a GIF file is checked and if not, the image is returned. MP4 is not
# returned, as they are not playable inside an HTML img tag

animation_types='gif'

# if animation_types is gif then when can generate a fast preview gif
# every second frame is skipped and the frame rate doubled
# to give quick preview, Default (no)
fast_gif=no

# default width of animation image. Be cautious when you increase this
# most mobile platforms give a very brief amount of time (in seconds) 
# to download the image.
# Given your ZM instance will be serving the image, it will anyway be slow
# Making the total animation size bigger resulted in the notification not 
# getting an image at all (timed out)
animation_width=640

# animation_retry_sleep refers to how long to wait before trying to grab
# frame information if it failed. animation_max_tries defines how many times it 
# will try and retrieve frames before it gives up
animation_retry_sleep=15
animation_max_tries=3

## --- MONITOR SPECIFIC CHANGES FROM HERE ON 
## --- Every param above can be repeated inside
## --- monitor specific sections and they will override
## --- settings above

## Monitor specific settings
#
# - Format:  [monitor-<mid>]

# Examples:
# Let's assume your monitor ID is 999
[monitor-999]
# my driveway
match_past_detections=no
wait=5

object_detection_pattern=(person|car|motorbike|bus|truck|boat)

# Advanced example - here we want anything except potted plant
# exclusion in regular expressions is not
# as straightforward as you may think, so 
# follow this pattern
# object_detection_pattern = ^(?!object1|object2|objectN)
# the characters in front implement what is 
# called a negative look ahead

# object_detection_pattern=^(?!potted plant|pottedplant|bench|broccoli)
#alpr_detection_pattern=^(.*x11)
#delete_after_analyze=no
#detection_pattern=.*
#import_zm_zones=yes

# polygon areas where object detection will be done.
# You can name them anything except the keywords defined in the optional
# params below. You can put as many polygons as you want per [monitor-<mid>]
# (see examples).

my_driveway_perimeter=306,356 1003,341 1074,683 154,715
some_other_area=0,0,200,300,700,900
# use license plate recognition for my driveway
# see alpr section later for more data needed
resize=no
detection_sequence=object,alpr


