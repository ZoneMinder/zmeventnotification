# Configuration file for object detection
#
# NOTE: ALL parameters here can be overridden
# on a per monitor basis if you want. Just
# add them under the correct monitors -> <id> section
#
# REMEMBER: Most of these are just variables. The place where all
# of this comes together is the ml_sequence and stream_sequence
# structures.

general:
  # Please don't change this. It is used by the config upgrade script
  version: "1.2"

  # You can now limit the # of detection process
  # per target processor. If not specified, default is 1
  # Other detection processes will wait to acquire lock
  cpu_max_processes: 3
  tpu_max_processes: 1
  gpu_max_processes: 1

  # Time to wait in seconds per processor to be free, before
  # erroring out. Default is 120 (2 mins)
  cpu_max_lock_wait: 100
  tpu_max_lock_wait: 100
  gpu_max_lock_wait: 100

  # If your ZoneMinder setup is using UNIX socket to connect
  # a local database and in your zm.conf there is a line that
  # looks like this one:
  # ZM_DB_HOST=localhost:/var/run/mysqld/mysqld.sock
  # Event Server won't play nice with it and zm_detect will fail.
  # Uncomment the following pyzm_overrides and properly populate
  # dbhost entry with hostname:port to get zm_detect working.
  #
  #pyzm_overrides:
  #  log_level_debug: 5
  #  dbhost: "localhost:3306"
  #
  # Use the following other configurations otherwise:
  #pyzm_overrides:
  #  conf_path: "/etc/zm"
  #  log_level_debug: 0
  pyzm_overrides:
    log_level_debug: 5

  # This is an optional file
  # If specified, you can specify tokens with secret values in that file
  # and only refer to the tokens in your main config file
  secrets: /etc/zm/secrets.ini

  # portal/user/password are needed if you plan on using ZM's legacy
  # auth mechanism to get images
  portal: "!ZM_PORTAL"
  user: "!ZM_USER"
  password: "!ZM_PASSWORD"

  # api portal is needed if you plan to use tokens to get images
  # requires ZM 1.33 or above
  api_portal: "!ZM_API_PORTAL"

  allow_self_signed: "yes"

  # if yes, last detection will be stored for monitors
  # and bounding boxes that match, along with labels
  # will be discarded for new detections. This may be helpful
  # in getting rid of static objects that get detected
  # due to some motion.
  match_past_detections: "no"

  # The max difference in area between the objects if match_past_detection is on
  # can also be specified in px like 300px. Default is 5%.
  # Note: You can specify label/object specific max_diff_areas as well.
  # example:
  # person_past_det_max_diff_area: "5%"
  # car_past_det_max_diff_area: "5000px"
  past_det_max_diff_area: "5%"

  # this is the maximum size a detected object can have.
  # You can specify it in px or % just like past_det_max_diff_area
  max_detection_size: "90%"

  # sequence of models to run for detection
  my_model_sequence: "object,face,alpr"

  # If you need basic auth to access ZM
  #basic_user: user
  #basic_password: password

  # base data path for various files the ES+OD needs
  # we support in config variable substitution as well
  base_data_path: /var/lib/zmeventnotification

  # global settings for
  # bestmatch, alarm, snapshot OR a specific frame ID
  frame_id: bestmatch

  # this is the to resize the image before analysis is done
  resize: "800"

  # set to yes, if you want to remove images after analysis
  # setting to yes is recommended to avoid filling up space
  # keep to no while debugging/inspecting masks
  # Note this does NOT delete debug images later
  delete_after_analyze: "yes"

  # If yes, will write an image called <filename>-bbox.jpg as well
  # which contains the bounding boxes. This has NO relation to
  # write_image_to_zm
  write_debug_image: "no"

  # if yes, will write an image with bounding boxes
  # this needs to be yes to be able to write a bounding box
  # image to ZoneMinder that is visible from its console
  write_image_to_zm: "yes"

  # Adds percentage to detections
  # hog/face shows 100% always
  show_percent: "yes"

  # color to be used to draw the polygons you specified
  poly_color: "(255,255,255)"
  poly_thickness: 2

  #import_zm_zones: "yes"
  only_triggered_zm_zones: "no"


# This section gives you an option to get brief animations
# of the event, delivered as part of the push notification to mobile devices
# Animations are created only if an object is detected
#
# NOTE: This will DELAY the time taken to send you push notifications
animation:
  create_animation: "no"

  # Format of animation burst
  # valid options are "mp4", "gif", "mp4,gif"
  animation_types: "mp4,gif"

  # default width of animation image
  animation_width: 640

  # When an event is detected, ZM writes frames a little late
  animation_retry_sleep: 15
  animation_max_tries: 4

  # if animation_types is gif then we can generate a fast preview gif
  fast_gif: "no"


remote:
  # You can now run the machine learning code on a different server
  # To do this, you need to setup https://github.com/pliablepixels/mlapi

  # URL that will be used
  #ml_gateway: "http://192.168.1.183:5000/api/v1"
  #ml_fallback_local: "yes"

  # API/password for remote gateway
  ml_user: "!ML_USER"
  ml_password: "!ML_PASSWORD"

  # timeout in seconds, time wait for mlapi response
  ml_timeout: 5


# config for object
object:
  object_detection_pattern: "(person|car|motorbike|bus|truck|boat)"
  object_min_confidence: 0.3
  object_framework: coral_edgetpu
  object_processor: tpu
  object_weights: "{{base_data_path}}/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite"
  object_labels: "{{base_data_path}}/models/coral_edgetpu/coco_indexed.names"

  # Google Coral
  tpu_object_weights_mobiledet: "{{base_data_path}}/models/coral_edgetpu/ssdlite_mobiledet_coco_qat_postprocess_edgetpu.tflite"
  tpu_object_weights_mobilenet: "{{base_data_path}}/models/coral_edgetpu/ssd_mobilenet_v2_coco_quant_postprocess_edgetpu.tflite"
  tpu_object_labels: "{{base_data_path}}/models/coral_edgetpu/coco_indexed.names"
  tpu_object_framework: coral_edgetpu
  tpu_object_processor: tpu
  tpu_min_confidence: 0.6

  # Yolo v4 on GPU (falls back to CPU if no GPU)
  yolo4_object_weights: "{{base_data_path}}/models/yolov4/yolov4.weights"
  yolo4_object_labels: "{{base_data_path}}/models/yolov4/coco.names"
  yolo4_object_config: "{{base_data_path}}/models/yolov4/yolov4.cfg"
  yolo4_object_framework: opencv
  yolo4_object_processor: gpu

  # Yolo v3 on GPU (falls back to CPU if no GPU)
  yolo3_object_weights: "{{base_data_path}}/models/yolov3/yolov3.weights"
  yolo3_object_labels: "{{base_data_path}}/models/yolov3/coco.names"
  yolo3_object_config: "{{base_data_path}}/models/yolov3/yolov3.cfg"
  yolo3_object_framework: opencv
  yolo3_object_processor: gpu

  # Tiny Yolo V4 on GPU (falls back to CPU if no GPU)
  tinyyolo_object_config: "{{base_data_path}}/models/tinyyolov4/yolov4-tiny.cfg"
  tinyyolo_object_weights: "{{base_data_path}}/models/tinyyolov4/yolov4-tiny.weights"
  tinyyolo_object_labels: "{{base_data_path}}/models/tinyyolov4/coco.names"
  tinyyolo_object_framework: opencv
  tinyyolo_object_processor: gpu

  # ONNX YOLOv11 nano via OpenCV DNN (GPU accelerated)
  onnx_v11n_object_weights: "{{base_data_path}}/models/ultralytics/yolo11n.onnx"
  onnx_v11n_object_labels: "{{base_data_path}}/models/yolov4/coco.names"
  onnx_v11n_object_framework: opencv
  onnx_v11n_object_processor: gpu

  # ONNX YOLOv11 small via OpenCV DNN (GPU accelerated)
  onnx_v11s_object_weights: "{{base_data_path}}/models/ultralytics/yolo11s.onnx"
  onnx_v11s_object_labels: "{{base_data_path}}/models/yolov4/coco.names"
  onnx_v11s_object_framework: opencv
  onnx_v11s_object_processor: gpu


face:
  face_detection_pattern: ".*"
  known_images_path: "{{base_data_path}}/known_faces"
  unknown_images_path: "{{base_data_path}}/unknown_faces"
  save_unknown_faces: "yes"
  save_unknown_faces_leeway_pixels: 100
  face_detection_framework: dlib
  face_num_jitters: 1
  face_model: cnn
  face_upsample_times: 1
  face_recog_dist_threshold: 0.6
  face_train_model: cnn
  #unknown_face_name: invader


alpr:
  alpr_detection_pattern: ".*"
  alpr_use_after_detection_only: "yes"
  # Many of the ALPR providers offer both a cloud version
  # and local SDK version. Set this to local or cloud.
  alpr_api_type: cloud

  # -----| If you are using plate recognizer | ------
  alpr_service: plate_recognizer
  #alpr_url: "http://192.168.1.21:8080/alpr"
  alpr_key: "!PLATEREC_ALPR_KEY"
  platerec_stats: "yes"
  #platerec_regions: ['us','cn','kr']
  platerec_min_dscore: 0.1
  platerec_min_score: 0.2

  # ----| If you are using openALPR |-----
  #alpr_service: open_alpr
  #alpr_key: "!OPENALPR_ALPR_KEY"
  #openalpr_recognize_vehicle: 1
  #openalpr_country: us
  #openalpr_state: ca
  #openalpr_min_confidence: 0.3

  # ----| If you are using openALPR command line |-----
  openalpr_cmdline_binary: alpr
  openalpr_cmdline_params: "-j -d"
  openalpr_cmdline_min_confidence: 0.3


ml:
  # if enabled, will not grab exclusive locks before running inferencing
  disable_locks: "no"

  # Chain of frames
  # See https://zmeventnotification.readthedocs.io/en/latest/guides/hooks.html#understanding-detection-configuration
  stream_sequence:
    frame_strategy: most_models
    frame_set: "snapshot,alarm"
    contig_frames_before_error: 5
    max_attempts: 3
    sleep_between_attempts: 4
    resize: 800

  # Chain of ML models to use
  # See https://zmeventnotification.readthedocs.io/en/latest/guides/hooks.html#understanding-detection-configuration
  ml_sequence:
    general:
      model_sequence: "{{my_model_sequence}}"
      disable_locks: "{{disable_locks}}"
      match_past_detections: "{{match_past_detections}}"
      past_det_max_diff_area: "5%"
      car_past_det_max_diff_area: "10%"
      #ignore_past_detection_labels: ['dog', 'cat']
      aliases:
        - ['car', 'bus', 'truck', 'boat']
        - ['broccoli', 'pottedplant']

    object:
      general:
        pattern: "{{object_detection_pattern}}"
        same_model_sequence_strategy: first
      sequence:
        - name: TPU object detection
          enabled: "no"
          object_weights: "{{tpu_object_weights_mobiledet}}"
          object_labels: "{{tpu_object_labels}}"
          object_min_confidence: "{{tpu_min_confidence}}"
          object_framework: "{{tpu_object_framework}}"
          tpu_max_processes: "{{tpu_max_processes}}"
          tpu_max_lock_wait: "{{tpu_max_lock_wait}}"
          max_detection_size: "{{max_detection_size}}"

        - name: YoloV4 GPU/CPU
          enabled: "yes"
          object_config: "{{yolo4_object_config}}"
          object_weights: "{{yolo4_object_weights}}"
          object_labels: "{{yolo4_object_labels}}"
          object_min_confidence: "{{object_min_confidence}}"
          object_framework: "{{yolo4_object_framework}}"
          object_processor: "{{yolo4_object_processor}}"
          gpu_max_processes: "{{gpu_max_processes}}"
          gpu_max_lock_wait: "{{gpu_max_lock_wait}}"
          cpu_max_processes: "{{cpu_max_processes}}"
          cpu_max_lock_wait: "{{cpu_max_lock_wait}}"
          max_detection_size: "{{max_detection_size}}"

        - name: ONNX YOLOv11n GPU/CPU
          enabled: "no"
          object_weights: "{{onnx_v11n_object_weights}}"
          object_labels: "{{onnx_v11n_object_labels}}"
          object_min_confidence: "{{object_min_confidence}}"
          object_framework: "{{onnx_v11n_object_framework}}"
          object_processor: "{{onnx_v11n_object_processor}}"
          gpu_max_processes: "{{gpu_max_processes}}"
          gpu_max_lock_wait: "{{gpu_max_lock_wait}}"
          cpu_max_processes: "{{cpu_max_processes}}"
          cpu_max_lock_wait: "{{cpu_max_lock_wait}}"
          max_detection_size: "{{max_detection_size}}"

    face:
      general:
        pattern: "{{face_detection_pattern}}"
        #pre_existing_labels: ['person']
        same_model_sequence_strategy: union
      sequence:
        - name: TPU face detection
          enabled: "no"
          face_detection_framework: tpu
          face_weights: /var/lib/zmeventnotification/models/coral_edgetpu/ssd_mobilenet_v2_face_quant_postprocess_edgetpu.tflite
          face_min_confidence: 0.3

        - name: DLIB based face recognition
          enabled: "yes"
          #pre_existing_labels: ['face']
          save_unknown_faces: "{{save_unknown_faces}}"
          save_unknown_faces_leeway_pixels: "{{save_unknown_faces_leeway_pixels}}"
          face_detection_framework: "{{face_detection_framework}}"
          known_images_path: "{{known_images_path}}"
          unknown_images_path: "{{unknown_images_path}}"
          face_model: "{{face_model}}"
          face_train_model: "{{face_train_model}}"
          face_recog_dist_threshold: "{{face_recog_dist_threshold}}"
          face_num_jitters: "{{face_num_jitters}}"
          face_upsample_times: "{{face_upsample_times}}"
          gpu_max_processes: "{{gpu_max_processes}}"
          gpu_max_lock_wait: "{{gpu_max_lock_wait}}"
          cpu_max_processes: "{{cpu_max_processes}}"
          cpu_max_lock_wait: "{{cpu_max_lock_wait}}"
          max_size: 800

    alpr:
      general:
        same_model_sequence_strategy: first
        pre_existing_labels: ['car', 'motorbike', 'bus', 'truck', 'boat']
        pattern: "{{alpr_detection_pattern}}"
      sequence:
        - name: Platerecognizer cloud
          enabled: "yes"
          alpr_api_type: "{{alpr_api_type}}"
          alpr_service: "{{alpr_service}}"
          alpr_key: "{{alpr_key}}"
          platrec_stats: "{{platerec_stats}}"
          platerec_min_dscore: "{{platerec_min_dscore}}"
          platerec_min_score: "{{platerec_min_score}}"
          max_size: 1600
          #platerec_payload:
          #  regions: ['us']
          #  camera_id: 12
          #platerec_config:
          #  region: strict
          #  mode: fast


## Monitor specific settings
# Examples:
# Let's assume your monitor ID is 999
monitors:
  999:
    # my driveway
    match_past_detections: "no"
    wait: 5
    object_detection_pattern: "(person)"
    resize: "no"
    my_model_sequence: "object,alpr"

    # polygon areas where object detection will be done.
    # Each zone has coords (polygon vertices) and an optional
    # detection_pattern (defaults to object_detection_pattern if omitted)
    zones:
      my_driveway:
        coords: "306,356 1003,341 1074,683 154,715"
        detection_pattern: "(person)"
      some_other_area:
        coords: "0,0 200,300 700,900"
